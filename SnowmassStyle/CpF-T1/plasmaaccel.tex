\subsection{Plasma based accelerators  for future colliders}

% this is new
Laser driven plasma accelerators  (LPAs) \cite{EsareyRMP09} and particle beam driven plasma accelerators (PWFAs) have the potential to reduce the size of future linacs for high energy physics by more than an order of magnitude, due to their high gradient.  High quality GeV beams have been produced by LPAs in 3 cm, while  energy gains of 40 GeV in a meter have been demonstrated in PWFA.   LPA research is in progress at facilities, including the BELLA PetaWatt laser at LBNL, towards high quality 10 GeV beams and staging of multiple modules, as well as control of injection and beam quality. PWFA research includes FACET at SLAC, which is exploring controllable acceleration of high quality electron/positron beams in a meter long PWFA.  These aim to address physics and R\&D challenges for a detailed design of future collider concepts (see white paper by J.P. Delahaye \textit{et al.}).  

Simulations must resolve plasma formation, driver beam propagation and energy transfer, the injection and evolution of high quality particle beams, and the loading of the plasma structure by the beam.  Core methods are explicit and implicit particle in cell and fluid.  These scale well but stretch computational abilities even for current experiments at the 1-10 GeV level in m-scale plasmas with 0.1 micron emittance (including loading of the plasma by the accelerating beam) \cite{web}.  

The path towards high-energy physics applications will likely involve hundreds of 10 GeV-scale stages with injectors, compact beam transport between stages, cooling, and focusing (e.g. adiabatic plasma lens) \cite{Leemans09}.  For collider emittances, this simultaneously increases the length of simulation and the accuracy with which beam emittance must be resolved by one to two orders of magnitude, while domain size increases only modestly.  Also required are simulations, self consistently with the plasma, of scattering, radiation, spin polarization and production of positrons (or other accelerated particles).  These in turn require increased particle number for statistics.  Control of injection or dephasing, or near-hollow channels to mitigate scattering-induced emittance growth, require that plasma formation codes be developed to account for 3D effects and self consistent laser heat deposition.   High average power at kHz-MHz repetition rates will require inclusion of target heat flow and laser modeling.

While scaling can increase particle number and resolution, needs for increased run length and accuracy with added physical models motivate new methods.  Recent examples include computation in a boosted frame, where the scale disparity is reduced, envelope codes which average over the laser period, and methods to reduce unphysical momentum spread.  Numerical methods with improved accuracy and reduced unphysical momentum contributions will be critical.  These may include Vlasov and/or models that exploit specific physics features (e.g. envelope, boost, r-z).  In particular, as compute power appears likely to increase faster than bandwidth, more accurate methods allowing longer timesteps (even if at higher computational cost) may be advantageous. Emerging multicore or SIMD systems function well with PIC codes, but development of common compilers and tools are a high priority for productivity.  Heterogeneous decomposition will also likely be required.

%Laser plasma accelerators (LPAs) \cite{EsareyRMP09} have the potential to reduce the size of future linacs for high energy physics by more than an order of magnitude, due to their high gradient.   Research is in progress at facilities, including the BELLA PetaWatt laser at LBNL, towards high quality 10 GeV beams and staging of multiple modules, as well as control of injection and beam quality.  This aims to address physics and R\&D challenges for a detailed design of future collider concepts (see white paper by J.P. Delahaye \textit{et al.}).  Particle beam driven plasmas are in the same paper.
%Simulations must resolve plasma formation, laser propagation and energy transfer, and the injection and evolution of high quality particle beams.  Core methods are explicit and implicit particle in cell and fluid.  These scale well but stretch computational abilities even for current experiments at the 1-10 GeV level in m-scale plasmas with 0.1 micron emittance ( including loading of the plasma by the accelerating beam)  \cite{web}.  
%The path towards high-energy physics applications will likely involve hundreds of 10 GeV-scale stages with injectors, compact beam cooling, and focusing (e.g. adiabatic plasma lens) \cite{Leemans09}.  For collider emittances, this simultaneously increases the length of simulation and the accuracy with which beam emittance must be resolved by one to two orders of magnitude, while domain size increases only modestly.  Also required are simulation, self consistently with the plasma, of scattering, radiation, spin polarization and production of positrons (or other accelerated particles).  These in turn require increased particle number for statistics.  Control injection or dephasing, and on near-hollow channels to mitigate scattering-induced emittance growth require that plasma formation codes be developed to account for 3D effects and self consistent laser heat deposition.   High average power at kHz-MHz repetition rates will also require inclusion of target heat flow and laser modeling.
%While scaling can increase particle number and resolution, needs for increased run length and accuracy with added physical models motivate new methods.  Recent examples include computation in a boosted frame, where the scale disparity is reduced, envelope codes which average over the laser period, and methods to reduce unphysical momentum spread.  Numerical methods with improved accuracy and reduced unphysical momentum contributions will be critical.  These may include Vlasov and/or models that exploit specific physics features (e.g. envelope, boost).  In particular, as compute power appears likely to increase faster than bandwidth, more accurate methods allowing longer timesteps (even if at higher computational cost) may be advantageous. Emerging multicore or SIMD systems function well with PIC codes, but development of common compilers and tools are a high priority for productivity.  Heterogeneous decomposition will also likely be required.

