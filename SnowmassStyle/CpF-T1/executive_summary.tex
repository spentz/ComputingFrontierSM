%%%%%% Computing Chapter  %%%%%%%%%%%%%%%%
 
\chapter{Computing Frontier: Accelerator Science}
\label{chap:mag}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}\begin{boldmath}

\input CpF-T1/authorlist.tex
%Conveners are also listed separately in authorlist.tex

\end{boldmath}\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Executive Summary}
\label{sec:comp-exec}

Particle accelerators are critical to scientific discovery, both nationally and worldwide. The development and optimization of accelerators are essential for advancing our understanding of the fundamental properties of matter, energy, space and time. Modeling of accelerator components and simulation of beam dynamics are necessary for understanding and optimizing the performance of existing accelerators, for optimizing the design and cost-effectiveness of future accelerators, and for discovering and developing new acceleration techniques and technologies.  In addition, the combination of fast and sofisticated analytics with large scale simulations will be very important to obtain the control-room feedback capabilities required by Intensity Frontier accelerators.

\subsection{Simulations} \label{subsec:accel-simu}
The requirements for high-fidelity computer simulations of accelerator systems and accelerator components are driven by the need to develop and optimize new accelerator concepts and design machines based on these concepts, and maximize the performance of existing accelerators based on existing concepts and technologies.  For  Energy Frontier applications this means supporting the development of new techniques that will increase the accelerating gradients so future machines are more compact and less costly. The options considered in our study include acceleration in plasma structures, using either laser- or beam-driven wakefields, dielectric structures driven by lasers or RF (GHz), the development of new lepton collider designs such as muon colliders and two-beam acceleration, and optimization of existing technologies such as superconducting rf cavities. For  Intensity Frontier applications, simulations are essential in developing and optimizing integrated designs in order to minimize beam losses.  Such loases are caused by instabilities generated either by beam self-interactions, or by interactions of the beam with the accelerator structures or other media present in the beam pipe.  The simulations considered in our study focused on designing mitigation techniques and determining optimal operational parameters.  Hadron colliders at the Energy Frontier have similar requirements, although self-interactions are not important, while beam-beam interactions (which are similarly computationally intensive) have to be included.  

Simulations of accelerators for both the Energy and the Intensity frontiers are computationally demanding because they often involve a wide range of time and length scales and a wide spectrum of interoperating physics components. For example, simulations of high-intensity proton drivers which are a few km long and operate using EM wavelengths of $10$-$100$ m,  with machine components of the order of $1$-$10$ m, must resolve particle bunches of the order of a few mm. Similarly, laser-plasma accelerators (LPA) of the order of $1$ m in length must resolve laser wavelength and electron bunch size of the order of $1$ $\mu$m.

Most of software for accelerator science are already  highly parallelized and scalable to $> 10$k cores on high performance computers. Modeling physical fields using various approximations requires different numerical methods. For example, electrostatic models utilze multigrid, Adaptive Mesh Refinment (AMR) multigrid or spectral methods, while various finite difference and finite element methods are best for fully electromagnetic models (finite element direct and hybrid, extended stencil finite difference, AMR finite difference). Quasi-static models require spectral methods, while Adaptive Mesh Refinment could be necessary in many of these models, as are matrix solvers, Particle in Cell, mesh generator and other libraries, and a variety of analysis tools. There are ongoing R\&D efforts to port these numerical models on new architectures such as GPU based machines.

The computational and computing needs for supporting Accelerator
Science are dominated by the need to efficiently take advantage of 
High-Performance-Computing (HPC) and the availability of tools
and resources that makes this possible.  Each
simulation step requires communication between thousands to
millions of processors and parallelism and fast
interconnect are essential to the computation. The projected computing  needs for all
the major modeling applications from both energy and intensity
frontiers are shown in Table~\ref{tab:CompNeeds}, where the values 
are based on the current performance of our codes on the Hopper supercomputer at
NERSC. Note that in this report we do not detail data storage and
networking needs (with one exception), because our area will not
drive the overall requirements, which are dominated by HEP
experiment needs.  We will leverage from the solutions
implemented to support these programs.

Our user community (accelerator scientists operating machines or performing R\&D) and our own community (computational accelerator physicists and theorists) identified the need for programmatic coordination and support of code development and computing R\&D to create a sustainable
computational accelerator science program as an essential requirement for the future.   Porting of our
algorithms and workflows to new computing architectures
(light-weight CPU plus accelerator) and the R\&D necessary to
create and evaluate new algorithms is an important component of
such coordinated program (including close interactions with HPC
centers to utilize test-beds of new architectures). An example of
such programmatic support of code development today is the SciDAC program, although
it is desirable that in the future there is more focus on the
specific physics solutions needed to further develop our tools.  Another
common theme is the need for supporting the development of
community libraries and tools, including standardized user
interfaces, geometry and data descriptions, I/O and analysis tools.
Because our applications require true HPC capabilities,
it is important to develop generic workflow tools that perform in an HPC
environment as well as on local workstations and clusters, as is the development and
integration to our toolkit of parameter optimization libraries,
that will be available across all HPC platforms.  The development
of such environment will enable experimentalists and machine
operators to take advantage of these computational capabilities
and will be essential in training students and young researchers
to help develop the new accelerator concepts and technologies
that will move the field of particle accelerators forward.   In
addition, it is essential for such program to support and
coordinate physics model validation and verification, ultimately
with comparisons to experimental data of well controlled
experiments in test facilities or operating accelerators.

\subsection{Feedback and Control Systems}
Intensity Frontier machines of the future require control room
feedback capabilities because of the beam-loss implications.  This 
capability is also important to Energy Frontier test
facilities for guiding and interpreting experiments.   The utilization of new computing technologies could make 
delivering  such a fast turnaround possible. The challenge on both the
performance of the computational tools and the availability of
computing resources becomes even  more daunting if we consider
the need to analyze the simulated data in order to extract useful
information.  The analysis of the simulated data ($\sim$ TB) has
to produce the same quantities observed by the beam diagnostic
detectors.  Note that this is a more general requirement, because
it is necessary for accurate comparisons of simulated and
observed data independently of the ability to do that in ``almost
real-time'' in the control room.  Analysis workflow
and synthetic diagnostic tools similar to those used by HEP
experiments have to be developed to properly model the detector
response and maintain and correlate the information of the
simulated physics variables to those "smeared" by modeling the
beam diagnostics.  Such analysis tools have to be HPC capable, to
allow for the fast turnaround necessary for control room
feedback, and they will also require new models
and algorithms.  Finally, this is probably the only application
in accelerator modeling that data transfer speed and data
availability, storage, and cataloging has similar requirements to
those of a  DAQ system for a HEP experiment.


\subsection{Multi-physics modeling}
 Different applications have different specific
requirements for the development of new or more efficient physics
or computational models, but all of them require integrated
multi-scale, multi-physics modeling.  
Although the physics models implemented in today's simulation tools utilize the  sophisticated HPC infrastructure, because of the size of the computation, often ``single physics'' or ``few physics" models are included in a run. The different physics effects are studied separately, as if they were independent.  This is not the case in general, affecting our ability to find optimal design and operational parameters.  More efforts are needed to integrate multiple physics for more accurate simulations, with the ability to utilize massive computing resources beyond the capabilities of today. 
   In the energy frontier, where single components of the accelerator are simulated separately, end-to-end simulations and integration between components is needed.  For example, plasma based accelerators simulations must be advanced from modeling current experiments at the 10 GeV and 0.1 micron emittance level to future collider concepts involving 100s of stages at the 0.01 micron emittance level, which also requires integration of additional physical models such as scattering and radiation. For high-intensity circular proton machines, a large number of macro-particles ($\sim 10^9$) must be used in the simulations in order to accurately represent \% level losses. In addition, detailed models of important components relevant to all frontier applications are missing from our simulation toolkits because of prohibitive computational cost and complexity (for example target modeling, including Gas dynamics, MHD, and heat loading/dissipation must be integrated to our toolkit). 
Deployment of such capabilities
will enable end-to-end simulations to validate designs based on
new concepts and end-to-end operational parameter optimization of
accelerators about to be commissioned.  It should be noted that
in some cases end-to-end modeling also involves integration of physics
and numerical models developed for different applications (for
example, for a plasma based accelerator consisting of many plasma
stages, both plasma physics tools and conventional beam-dynamics
tools have to be used in the model to produce an optimal
solution).

\subsection{Design optimization}
Intensity Frontier accelerator needs are dominated by the need to
control and mitigate beam losses.  This demands both careful
design of the accelerator structures and accurate modeling of
beam-halo (and its creation mechanisms) and the accelerator
geometry (apertures) and accelerator elements fields and
positions.  This implies tracking many bunches of $\sim 10^9$
macroparticles per bunch for $\sim 10^5$ turns including
self-fields, impedance effects, and bunch-to-bunch interactions.
Finding the optimal parameters of operation will require
end-to-end  optimization runs, while developing mitigation
techniques possibly requires the implementation of new physics in the HPC
environment, to model the new components (for example, electron
lenses for space-charge compensation). Energy Frontier
application based on protons have similar modeling needs for loss
control and mitigation, although in this case impedance effects
dominate (and possibly beam-beam interactions in a collider) as
self-interactions are not important.

Energy Frontier accelerator needs are dominated by the need to
develop end-to-end simulations to characterize and optimize beam
stability and emittance and transport efficiency.  New
accelerator concepts have many specific new physics model
capability needs, for example development of electromagnetic
plasma and beam methods capable of resolving 0.1 km-scale
propagation of 10 nm scale emittance bunches and laser drivers,
and the corresponding bunch conditioning and focusing, but there
are also a lot of common needs.  For example, radiation and
scattering, which is relevant to muon collider, plasma and
gamma-gamma options, and modeling of targets,  including Gas
dynamics, MHD, and heat loading/dissipation,  which is relevant
to both EF and ID applications.  Developing these new models
demands R\&D both on the physics and numerical algorithm area.
Because of the physics requirements imposed by some of the new
concepts considered, minimization of numerical noise is very
important in these applications.  This constraint has a direct
impact on the choices of numerical techniques for different
physics implementations.  Plasma accelerators additionally require 
computation of these effects with accurate plasma and laser dynamics,
 often requiring unique algorithms.

\begin{table}[t]
\begin{center}
\begin{tabular}{|l|l|} 
 \hline 
 Computation (Mhours) & $15000$ \\ \hline
 Typical cores for production runs & $50000$ \\ \hline
 Maximum cores for production runs & $5$M\\ \hline
 Data read and written per run (TB) & $1000$\\ \hline
 Minimum I/O bandwidth & $100$ GB/sec\\ \hline
 Memory requirement per core & $0.2$ GB \\ \hline
Shared file-system space (on site) & $6$ PB\\ \hline
Shared file-system space (distributed, cataloged) & $60$ PB\\ \hline
\end{tabular}
\caption{Compute needs in 10 years.}
\label{tab:CompNeeds}
\end{center}
\end{table}





%\section{Summary}
%\label{sec:comp-summary}



\end{document}
