%%%%%% Computing Chapter  %%%%%%%%%%%%%%%%
 
\chapter{Computing Frontier: Accelerator Science}
\label{chap:mag}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}\begin{boldmath}

\input CpF-T1/authorlist.tex
%Conveners are also listed separately in authorlist.tex

\end{boldmath}\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Executive Summary}
\label{sec:comp-exec}

Particle accelerators are critical to scientific discovery both nationally and worldwide. The development and optimization of accelerators are essential for advancing our understanding of the fundamental properties of matter, energy, space and time. Modeling of accelerator components and simulation of beam dynamics are necessary for understanding and optimizing the performance of existing accelerators, for optimizing the design and cost effectiveness of future accelerators, and for discovering and developing new acceleration techniques and technologies.

The requirements for high-fidelity computer simulations of accelerator systems and accelerator components are driven by the need to develop and optimize new accelerator concepts and design machines based on these concepts, and maximize the performance of accelerators based on existing concepts and technologies.  For  Energy Frontier applications this means supporting the development of new techniques that will increase the accelerating gradients so future machines are more compact and less costly. The options considered in our study include acceleration in plasma structures, using either laser or beam driven wakefields, dielectric structures driven by lasers or RF (GHz), the development of new lepton collider designs such as muon colliders and two-beam acceleration, and optimization of existing technologies such as superconducting rf cavities. For  Intensity Frontier, simulations are essential in developing and optimizing integrated designs in order to minimize beam losses due to instabilities caused either by beam self-interactions or by interactions of the beam with the accelerator structures or other media present in the beam pipe.  This  includes both designing mitigation techniques and determining optimal operational parameters.  Hadron colliders at the Energy Frontier have similar requirements, although self-interactions are not important and beam-beam interactions (which are similarly computationally intensive) have to be included.  

Simulations of accelerators for both the Energy and the Intensity frontier are computationally demanding because they often involve a wide range of time and length scales and a wide spectrum of interoperating physics components. For example, high intensity proton drivers of the order of $10^3$ m, operating at an EM wavelength of $10^2$--$10$ m with components of the order of $10$--$1$ m must resolve particle bunches of the order of $10^{-3}$ m. Similarly, laser-plasma accelerators (LPA) of the order of $1$ m in length must resolve laser wavelength and electron bunch size of the order of $1$ $\mu$m.

Most of software for accelerator science are already  highly parallelized and scalable to $> 10$k cores on High Performance Computer. They use a wide variety of numerical models, such as electrostatic (multigrid, AMR multigrid, spectral), electromagnetic (finite difference, finite element direct and hybrid, extended stencil finite-difference, AMR finite-difference), quasi-static (spectral), and  matrix solvers, Particle in Cell, meshin and other libraries, and a variety of analysis tools. In addition, there are ongoing R\&D efforts to port these numerical models on new architectures such as GPU based machines.

The computational and computing needs for supporting Accelerator
Science are dominated by the need to optimally utilize
High-Performance-Computing (HPC) and the availability of tools
and resources that makes this utilization possible.  Here HPC is
defined in the conventional way, where parallelism and fast
interconnect is essential to the computation, since each
simulation step requires communication between thousands to
millions of processors. The projected computing  needs for all
the major modeling applications from both energy and intensity
frontiers is shown in Table~\ref{tab:CompNeeds}, where the units
are based on the current performance of our codes on Hopper at
NERSC. Note that in this report we do not detail data storage and
networking needs (with one exception), because our area will not
drive the overall requirements, which are dominated by HEP
experiment needs.  We will leverage from the solutions
implemented to support these programs.

A common theme from the requirements communicated both by our
user community (accelerator scientists operating machines or
performing R\&D at test facilities) and computational accelerator
physicists, is the need for programmatic coordination and support
of code development and computing R\&D to create a sustainable
computational accelerator science program.  Porting of our
algorithms and workflows to new computing architectures
(light-weight CPU plus accelerator) and the R\&D necessary to
create and evaluate new algorithms is an important component of
such coordinated program (including close interactions with HPC
centers to utilize test-beds of new architectures). An example of
such programmatic support today is the SciDAC program, although
it is desirable that in the future there is more focus on the
specific physics solutions needed to further develop our tools.  Another
common theme is the need for supporting the development of
community libraries and tools, including standardized user
interfaces, geometry and data descriptions, I/O and analysis tools.
Because our applications require true HPC capabilities,
development of generic workflow tools that perform in an HPC
environment as well as local workstations and clusters is very important, as is the development and
integration to our toolkit of parameter optimization libraries,
that will be available across all HPC platforms.  The development
of such environment will enable experimentalists and machine
operators to take advantage of these computational capabilities
and will be essential in training students and young researchers
to help develop the new accelerator concepts and technologies
that will move the field of particle accelerators forward.   In
addition, it is essential for such program to support and
coordinate physics model validation and verification, ultimately
with comparisons to experimental data of well controlled
experiments in test facilities or operating accelerators.


Intensity Frontier machines of the future require control room
feedback capabilities (because of the loss implications), a
capability that is also important to Energy Frontier test
facilities (for guiding and interpreting experiments).   Would it
be possible with utilization of new computing technologies to
deliver such fast turnaround?  The challenge on both the
performance of the computational tools and the availability of
computing resources becomes even  more daunting if we consider
the need to analyze the simulated data in order to extract useful
information.  The analysis of the simulated data ($\sim$ TB) has
to produce the same quantities observed by the beam diagnostic
detectors.  Note that this is a more general requirement, because
it is necessary for accurate comparisons of simulated and
observed data independently of the ability to do that in ``almost
real-time'' in the control room.  The necessary analysis workflow
and synthetic diagnostic tools similar to those used by HEP
experiments has to be developed to properly model the detector
response and maintain and correlate the information of the
simulated physics variables to those smeared by the model of the
diagnostics.  Such analysis tools have to be HPC capable, to
allow for the fast turnaround necessary for control room
feedback, and they will also require development of new models
and algorithms.  Finally, this is probably the only application
in accelerator modeling that data transfer speed and data
availability, storage, and cataloging has similar requirements to
those of a HEP experiment DAQ system.



 Different applications have different specific
requirements for the development of new or more efficient physics
or computational models, but all of them require integrated
multi-scale, multi-physics modeling.  
Although the physics models implemented in today's simulation tools utilize the  sophisticated HPC infrastructure, because of the size of the computation, often ``single physics'' or ``few physics" models are included in a run. The different physics effects are studied separately, as if they were independent.  This is not the case in general, affecting our ability to find optimal design and operational parameters.  More efforts are needed to integrate multiple physics for more accurate simulations, with the ability to utilize massive computing resources beyond the capabilities of today. 
   In the energy frontier, where single components of the accelerator are simulated separately, end-to-end simulations and integration between components is needed.  For example, plasma based accelerators simulations must be advanced from modeling current experiments at the 10 GeV and 0.1 micron emittance level to future collider concepts involving 100s of stages at the 0.01 micron emittance level, which also requires integration of additional physical models such as scattering and radiation. For high-intensity circular proton machines, a large number of macro-particles ($\sim 10^9$) must be used in the simulations in order to accurately represent \% level losses. In addition, detailed models of important components relevant to all frontier applications are missing from our simulation toolkits because of prohibitive computational cost and complexity (for example target modeling, including Gas dynamics, MHD, and heat loading/dissipation must be integrated to our toolkit). 
Deployment of such capabilities
will enable end-to-end simulations to validate designs based on
new concepts and end-to-end operational parameter optimization of
accelerators about to be commissioned.  It should be noted that
in some cases end-to-end modeling also involves integration of physics
and numerical models developed for different applications (for
example, for a plasma based accelerator consisting of many plasma
stages, both plasma physics tools and conventional beam-dynamics
tools have to be used in the model to produce an optimal
solution).

Intensity Frontier accelerator needs are dominated by the need to
control and mitigate beam losses.  This demands both careful
design of the accelerator structures and accurate modeling of
beam-halo (and its creation mechanisms) and the accelerator
geometry (apertures) and accelerator elements fields and
positions.  This implies tracking many bunches of $\sim 10^9$
macroparticles per bunch for $\sim 10^5$ turns including
self-fields, impedance effects, and bunch-to-bunch interactions.
Finding the optimal parameters of operation will require
end-to-end  optimization runs, while developing mitigation
techniques possibly requires the implementation of new physics in the HPC
environment, to model the new components (for example, electron
lenses for space-charge compensation). Energy Frontier
application based on protons have similar modeling needs for loss
control and mitigation, although in this case impedance effects
dominate (and possibly beam-beam interactions in a collider) as
self-interactions are not important.

Energy Frontier accelerator needs are dominated by the need to
develop end-to-end simulations to characterize and optimize beam
stability and emittance and transport efficiency.  New
accelerator concepts have many specific new physics model
capability needs, for example development of electromagnetic
plasma and beam methods capable of resolving 0.1 km-scale
propagation of 10 nm scale emittance bunches and laser drivers,
and the corresponding bunch conditioning and focusing, but there
are also a lot of common needs.  For example, radiation and
scattering, which is relevant to muon collider, plasma and
gamma-gamma options, and modeling of targets,  including Gas
dynamics, MHD, and heat loading/dissipation,  which is relevant
to both EF and ID applications.  Developing these new models
demands R\&D both on the physics and numerical algorithm area.
Because of the physics requirements imposed by some of the new
concepts considered, minimization of numerical noise is very
important in these applications.  This constraint has a direct
impact on the choices of numerical techniques for different
physics implementations.  Plasma accelerators additionally require 
computation of these effects with accurate plasma and laser dynamics,
 often requiring unique algorithms.

\begin{table}[t]
\begin{center}
\begin{tabular}{|l|l|} 
 \hline 
 Computation (Mhours) & $15000$ \\ \hline
 Typical cores for production runs & $50000$ \\ \hline
 Maximum cores for production runs & $5$M\\ \hline
 Data read and written per run (TB) & $1000$\\ \hline
 Minimum I/O bandwidth & $100$ GB/sec\\ \hline
 Memory requirement per core & $0.2$ GB \\ \hline
Shared file-system space (on site) & $6$ PB\\ \hline
Shared file-system space (distributed, cataloged) & $60$ PB\\ \hline
\end{tabular}
\caption{Compute needs in 10 years.}
\label{tab:CompNeeds}
\end{center}
\end{table}





%\section{Summary}
%\label{sec:comp-summary}



\end{document}
